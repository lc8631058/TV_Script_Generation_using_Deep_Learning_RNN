{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YEAR DATE 1989] Â© Twentieth Century Fox Film Corporation. All rights reserved.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "data_dir = \"./data/simpsons/moes_tavern_lines.txt\"\n",
    "text = helper.load_data(dir)\n",
    "print(text[:81])\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set stats\n",
      "The number of words in text: 11492\n",
      "Number of scene: 262\n",
      "Average Number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average words in sentences: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "import numpy as np\n",
    "print(\"Data set stats\")\n",
    "print(\"The number of words in text: {}\".format(len({word:None for word in text.split()})))\n",
    "\n",
    "scenes = text.split(\"\\n\\n\")\n",
    "print(\"Number of scene: {}\".format(len(scenes)))\n",
    "\n",
    "sentence_count_scene = [scene.count(\"\\n\") for scene in scenes]\n",
    "print(\"Average Number of sentences in each scene: {}\".format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split(\"\\n\")]\n",
    "print(\"Number of lines: {}\".format(len(sentences)))\n",
    "\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print(\"Average words in sentences: {}\".format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print(\"The sentences {} to {}\".format(*view_sentence_range))\n",
    "print(\"\\n\".join(text.split(\"\\n\")[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import problem_unittests as test\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    text_counter = Counter(text)\n",
    "    text_counter = sorted(text_counter, key=text_counter.get, reverse=True)\n",
    "    vocab_to_int = {word:idx for idx, word in enumerate(text_counter)}\n",
    "    int_to_vocab = {idx:word for idx, word in enumerate(text_counter)}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Tokenize = { '.':'||Period||', ',':'||Comma||', '\"':'||Quotation_Mark||', ';':'||Semicolon||',\\\n",
    "                '!':'||Exclamation_mark||', '?':'||Question_mark||', '(':'||Left_Parentheses||', \\\n",
    "                ')':'||Right_Parentheses||', '--':'||Dash||', '\\n':'||Return||' }\n",
    "                \n",
    "    return Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name=\"input\")\n",
    "    Targets = tf.placeholder(tf.int32, [None,None], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.int32, name=\"learning_rate\")\n",
    "    return (inputs, Targets, LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, num_layers):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell([lstm]*num_layers)\n",
    "    initial_state = multi_cell.zero_state(batch_size, tf.float32)\n",
    "    # to create the cells but with name\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    \n",
    "    return (multi_cell, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding_matrix = tf.random_uniform([vocab_size, embed_dim], minval=-1, maxval=1)\n",
    "    embeded_input = tf.nn.embedding_lookup(embedding_matrix, input_data)\n",
    "    \n",
    "    return embeded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state= tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    \n",
    "    # give final_state name\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    \n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embeded_input = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embeded_input)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=tf.nn.relu)\n",
    "    return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
